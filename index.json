
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"My core interest in the human mind and brain lies in the bidirectional link between action and perception. In particular, I study how active observers select and integrate visual information and how actions orchestrate memory maintenance. In our research, we show how eye movements and accompanying shifts of attention shape what we see and what we remember—typically by setting strong spatial priorities. In a new line of research, I use adaptation to localize and dissect the mechanisms underlying causal perception. See also my personal website https://svenohl.wordpress.com.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4bb0048deaca203c43f52969e9ebf18e","permalink":"https://mindinaction.github.io/author/sven-ohl/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sven-ohl/","section":"authors","summary":"My core interest in the human mind and brain lies in the bidirectional link between action and perception. In particular, I study how active observers select and integrate visual information and how actions orchestrate memory maintenance.","tags":null,"title":"Sven Ohl","type":"authors"},{"authors":null,"categories":null,"content":"We are currently recruiting.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3b3a061865f77335949f7ad6efbb7be2","permalink":"https://mindinaction.github.io/author/open-phd-position/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/open-phd-position/","section":"authors","summary":"We are currently recruiting.","tags":null,"title":"Open PhD-position","type":"authors"},{"authors":null,"categories":null,"content":"We are currently recruiting.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"89fe01e96e081a7fcab803da0f41939f","permalink":"https://mindinaction.github.io/author/open-shk-position/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/open-shk-position/","section":"authors","summary":"We are currently recruiting.","tags":null,"title":"Open SHK-position","type":"authors"},{"authors":[],"categories":null,"content":"Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://mindinaction.github.io/event/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/event/example/","section":"event","summary":"An example event.","tags":[],"title":"Example Event","type":"event"},{"authors":["Rolfs","Schweitzer","Castet","Watson","Ohl"],"categories":null,"content":"","date":1704240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704240000,"objectID":"1ad80ea23a28c7447fdbcd2ce369b8ca","permalink":"https://mindinaction.github.io/publication/rolfs.2024/","publishdate":"2024-01-03T00:00:00Z","relpermalink":"/publication/rolfs.2024/","section":"publication","summary":"Perception relies on active sampling of the environment. What part of the physical world can be sensed is limited by biophysical constraints of sensory systems, but might be further constrained by the kinematic bounds of the motor actions that acquire sensory information. We tested this fundamental idea for humans’ fastest and most frequent behavior—saccadic eye movements—which entails retinal motion that commonly escapes visual awareness. We discover that the visibility of a high-speed stimulus, presented during fixation, is predicted by the lawful sensorimotor contingencies that saccades routinely impose on the retina, reflecting even distinctive variability between observers’ movements. Our results suggest that the visual systems’ functional and implementational properties are best understood in the context of movement kinematics that impact its sensory surface.","tags":null,"title":"Lawful kinematics link eye movements to the limits of high-speed perception","type":"publication"},{"authors":["Ohl","Rolfs"],"categories":null,"content":"","date":1704153600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704153600,"objectID":"c82bdb5339850ee211fcdbc42a93fec7","permalink":"https://mindinaction.github.io/publication/ohl.elife.2024/","publishdate":"2024-01-02T00:00:00Z","relpermalink":"/publication/ohl.elife.2024/","section":"publication","summary":"Detecting causal relations structures our perception of events in the world. Here, we determined whether generalized or specialized visual routines underly the perception of causality by assessing the adaptability of specific features in launching events of simple geometric shapes. After prolonged exposure to causal launch events (the adaptor) defined by a particular set of features (i.e., a particular motion direction, motion speed, or feature conjunction), observers were less likely to see causal interactions in subsequent ambiguous test events. We assessed whether this negative aftereffect transfers to test events with a new set of feature values that were not presented during adaptation. Processing in specialized (as opposed to generalized) visual routines predicts that the transfer of adaptation depends on the feature-similarity of the adaptor and the test event. We show that negative aftereffects do not transfer to unadapted launch directions. Crucially, adaptation was contingent on the causal impression in launches as demonstrated by a lack of adaptation in non-causal control events. In contrast, adaptation to launches with a particular motion speed transferred also to a different speed. Moreover, adaptation based on feature conjunctions (color and launch direction) revealed that launch direction trumps the feature identity of the object for causal perception; the adaptation transferred across colors if the test event had the same motion direction as the adaptor. In summary, visual adaptation allowed us to carve out a visual feature space underlying the perception of causality and revealed specialized visual routines that are tuned to a launch’s motion direction.","tags":null,"title":"Visual routines for detecting causal interactions are tuned to motion direction","type":"publication"},{"authors":null,"categories":null,"content":"We have a website. If you are interested in studying the interplay between actions and memory, or the perception of causality, please consider joining the lab in Berlin. Watch this space for new positions over the next couple of months.\nI will be more than happy to discuss possibilities for you to join us.\n","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"74cc2cb59c4f00eeb42ad2c0e52e62ba","permalink":"https://mindinaction.github.io/post/24-01-01-the-website/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/post/24-01-01-the-website/","section":"post","summary":"We have a website. If you are interested in studying the interplay between actions and memory, or the perception of causality, please consider joining the lab in Berlin. Watch this space for new positions over the next couple of months.\n","tags":null,"title":"Launch of the lab website","type":"post"},{"authors":null,"categories":null,"content":"This is the playground to test local changes.\nAnd implement different settings.\n","date":1700438400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700438400,"objectID":"585b8144115c5f114269f912f3443988","permalink":"https://mindinaction.github.io/extra/","publishdate":"2023-11-20T00:00:00Z","relpermalink":"/extra/","section":"","summary":"This is the playground to test local changes.\n","tags":null,"title":"Playground","type":"page"},{"authors":["Ohl","Kroell","Rolfs"],"categories":null,"content":"","date":1696118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696118400,"objectID":"a5f4ec203d47803cfa525b1257fb0d7e","permalink":"https://mindinaction.github.io/publication/ohl.jepgen.2023/","publishdate":"2023-10-01T00:00:00Z","relpermalink":"/publication/ohl.jepgen.2023/","section":"publication","summary":"Visual working memory and actions are closely intertwined. Memory can guide our actions, but actions also impact what we remember. Even during memory maintenance, actions such as saccadic eye movements select content in visual working memory, resulting in better memory at locations that are congruent with the action goal as compared to incongruent locations. Here, we further substantiate the claim that saccadic eye movements are fundamentally linked to visual working memory by analyzing a large data set (\u003e 100k trials) of nine experiments (eight of them previously published). Using Bayesian hierarchical models, we demonstrate robust saccadic selection across the full range of probed saccade directions, manifesting as better memory performance at the saccade goal irrespective of its location in the visual field. By inspecting individual differences in saccadic selection, we show that saccadic selection was highly prevalent in the population. Moreover, both saccade metrics and visual working memory performance varied considerably across the visual field. Crucially, however, both idiosyncratic and systematic visual field anisotropies were not correlated between visual working memory and the oculomotor system, suggesting that they resulted from different sources (e.g., rely on separate spatial maps). In stark contrast, trial-by-trial variations in saccade metrics were strongly associated with memory performance. At any given location, shorter saccade latencies and more accurate saccades were associated with better memory performance, undergirding a robust link between action selection and visual memory.","tags":null,"title":"Saccadic selection in visual working memory is robust across the visual field and linked to saccade metrics: Evidence from nine experiments and more than 100,000 trials","type":"publication"},{"authors":["Klotzsche","Gaebler","Villringer","Sommer","Nikulin","Ohl"],"categories":null,"content":"","date":1688256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688256000,"objectID":"771741aaf7f3ade7e813fd00134e18a5","permalink":"https://mindinaction.github.io/publication/klotzsche.psychophys.2023/","publishdate":"2023-07-02T00:00:00Z","relpermalink":"/publication/klotzsche.psychophys.2023/","section":"publication","summary":"Virtual reality (VR) offers a powerful tool for investigating cognitive processes, as it allows researchers to gauge behaviors and mental states in complex, yet highly controlled, scenarios. The use of VR head-mounted displays in combination with physiological measures such as EEG presents new challenges and raises the question whether established findings also generalize to a VR setup. Here, we used a VR headset to assess the spatial constraints underlying two well-established EEG correlates of visual short-term memory:the amplitude of the contralateral delay activity (CDA) and the lateralization of induced alpha power during memory retention. We tested observers' visual memory in a change detection task with bilateral stimulus arrays of either two or four items while varying the horizontal eccentricity of the memory arrays (4, 9, or 14 degrees of visual angle). The CDA amplitude differed between high and low memory load at the two smaller eccentricities, but not at the largest eccentricity. Neither memory load nor eccentricity significantly influenced the observed alpha lateralization. We further fitted time-resolved spatial filters to decode memory load from the event-related potential as well as from its time-frequency decomposition. Classification performance during the retention interval was above-chance level for both approaches and did not vary significantly across eccentricities. We conclude that commercial VR hardware can be utilized to study the CDA and lateralized alpha power, and we provide caveats for future studies targeting these EEG markers of visual memory in a VR setup.","tags":null,"title":"Visual short-term memory-related EEG components in a virtual reality setup","type":"publication"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://mindinaction.github.io/contact/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"Contact","type":"landing"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://mindinaction.github.io/people/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"People","type":"landing"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"b0d61e5cbb7472bf320bf0ef2aaeb977","permalink":"https://mindinaction.github.io/tour/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/tour/","section":"","summary":"","tags":null,"title":"Tour","type":"landing"},{"authors":["Rolfs","Ohl"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"55c582d77ef8655b30f7bf67b85ecbb0","permalink":"https://mindinaction.github.io/publication/rolfs.bbs.2021/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/rolfs.bbs.2021/","section":"publication","summary":"In active agents, sensory and motor processes form an inevitable bond. This wedding is particularly striking for saccadic eye movements—the prime target of Shadmehr and Ahmed’s thesis—which impose frequent changes on the retinal image. Changes in movement vigor (latency and speed), therefore, will need to be accompanied by changes in visual and attentional processes. We argue that the mechanisms that control movement vigor may also enable vision to attune to changes in movement kinematics.","tags":null,"title":"Moving fast and seeing slow? The perceptual consequences of vigorous movement","type":"publication"},{"authors":["Heuer","Ohl","Rolfs"],"categories":null,"content":"","date":1588032000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588032000,"objectID":"23815f73894cd7120296d2bf3c426688","permalink":"https://mindinaction.github.io/publication/heuer.viscog.2020/","publishdate":"2020-04-28T00:00:00Z","relpermalink":"/publication/heuer.viscog.2020/","section":"publication","summary":"Perception is shaped by actions, which determine the allocation of selective attention across the visual field. Here, we review evidence that maintenance in visual working memory is similarly influenced by actions (eye or hand movements), planned and executed well after encoding. Representations that are relevant for an upcoming action – because they spatially correspond to the action goal or because they are defined along action-related feature dimensions – are automatically prioritised over action-irrelevant representations and held in a stable state. We summarise what is known about specific characteristics and mechanisms of selection-for-action in working memory, such as its temporal dynamics and spatial specificity, and delineate open questions. This newly-burgeoning area of research promotes a more functional perspective on visual working memory that emphasizes its role in action control.","tags":null,"title":"Memory for action: A functional view of selection in visual working memory","type":"publication"},{"authors":["Ohl","Rolfs"],"categories":null,"content":"","date":1581724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581724800,"objectID":"bfef6cbe87750117dd8be98fe2999baf","permalink":"https://mindinaction.github.io/publication/ohl.jvis.2020/","publishdate":"2020-02-15T00:00:00Z","relpermalink":"/publication/ohl.jvis.2020/","section":"publication","summary":"Selection for visual short-term memory (vstm) provides a basis for many cognitive functions. Saccadic eye movements sway this selection in favor of stimuli previously seen at locations congruent with their target. In three experiments, we provide converging evidence that this saccadic selection is implemented as a fundamental, inevitable selection process, rather than a top-down strategy. In particular, benefits for congruent over incongruent items were largely constant across set sizes ranging from two to eight items (Experiment 1), showing that saccadic selection imposes priorities on vstm irrespective of memory load and is effective even when only few representations need to be maintained. Moreover, a decrement in performance for incongruent items occurred reliably, whether the congruent location contained a task-relevant item or an irrelevant noise patch (Experiment 2). Finally, saccadic selection was immune to a strong manipulation of the observer's attentional priorities (Experiment 3). Given the prevalence of saccades in natural vision, our results demonstrate a fundamental and ecologically relevant selection mechanism for vstm. Saccades systematically eliminate information seen at non-target locations, while information at the saccade target remains available to recall. This simple heuristic is effective in the absence of informative cues and may incapacitate voluntary selection mechanisms that are incongruent with ongoing movement plans.","tags":null,"title":"Bold moves: Inevitable saccadic selection in visual short-term memory","type":"publication"}]